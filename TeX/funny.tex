%!TEX root = main.tex

\section{Background}
Breast cancer is the number one most common cancer amongst women in Singapore. Early and accurate diagnosis of breast cancer is important for breast-saving and life-saving treatment. \\

The gold standard for the diagnosis of breast cancer is by surgically removing the breast lump with a complete microscopic examination of the breast tissue to look for cancer cells. \\

Fine needle aspiration is an alternative that allows the doctor to take out a small amount of tissue from the breast lump, without the need for surgery to remove the entire breast lump. By examining the characteristics of the cells, doctors have been able to diagnose breast cancer with variable success. Increasing the success of fine needle aspiration allows for diagnosis of breast cancer without the need for a woman to undergo surgery to remove the breast lump. \\

To resolve this, this project uses a fuzzy decision tree to classify breast tumor cells into malignant cancer cells or benign non-cancerous cells. \\

\section{Dataset}
The dataset used was the Breast Cancer Wisconsin (Diagnostic) Data Set from the University of Irvine (UCI) Machine Learning Repository.  

The dataset contained 569 instances, with no missing data. 357 instances were benign (not cancerous) and 212 were malignant (cancerous). \\

The features were computed from digitalized images of fine needle aspirates of breast tumors.  \\

The features describe 10 characteristics of the cell nuclei present in the images:

\begin{itemize}
	\item The radius of an individual nucleus
	\item The nuclear perimeter
	\item The nuclear area
	\item Compactness of the nucleus
	\item The smoothness of the contour of the nucleus
	\item The number of contour concavities
	\item The symmetry of the nuclear contour
	\item The texture of the cell nucleus
\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.45\linewidth]{screenshot001}
	\hspace{0.5cm}
	\includegraphics[width=0.45\linewidth]{screenshot002}
	\caption{A picture of breast cells. The cells on the left are begin while the cells on the right are cancerous.}
	\label{fig:images}
\end{figure}

\newpage

The mean, standard error and worst (mean of the three largest values) of these features are computed for each image, resulting in 30 features in the UCI dataset. \\

\iffalse
We follow in the footsteps of Sizilio et al. \ref{fna} and add newly generated features of homogeneity and uniformity that were demonstrated to have diagnostic importance. \\

Uniformity is the difference between the radius worst value and the radius mean value and is an indication of the variability in size of the cell nuclei. \\

Homogeneity is the difference between the worst value of symmetry and the mean value of symmetry and is an indication of the symmetry of the cell nuclei. \\

According to Sizilio et al., the features of area, perimeter, homogeneity and uniformity produced the best results. Thus, we use these 4 feature and drop all other features. \\

The minimum and maximum area, perimeter, homogeneity and uniformity for the 2 labels (benign and malignant) are computed and displayed in the table below. \\

Fuzzy intervals are present for each of the above 4 features, whereby the benign values are within the range of the malignant values. This means that it is not linearly possible to diagnosis a breast lump as benign or malignant using a simple decision tree. Thus, we shall resort to using a fuzzy decision tree. \\

\section{Fuzzy Decision Tree}
A typical decision tree works by building a set of if statements to determine which class something belongs to. \\

At each branch of the decision tree, depending on whether a given statement is true / false, either the left or right path is chosen. \\

On the other hand, a fuzzy decision tree makes use of fuzzy logic, where the truth value of variables can range between 0 and 1 instead of being exactly 0 or exactly 1. \\
\fi

\section{Decision Tree}
To solve this problem, we will make use of a decision tree. \\

\input{fig/decision_tree}

\heading{Entropy} A decision tree essentially asks certain questions at the right times, in order to determine which class the input belongs to. To do this, a value known as the entropy with the following formula is used.

$$E = \sum_{i=1}^{C}p_i\log_2 p_i$$

It measures the "impurity" of a set of objects. When the set is homogeneous (i.e. all the objects are of the same class), the entropy will be low. When the set is heterogeneous (i.e. the objects are from different classes), the entropy will be high. \\

\heading{Information Gain} In order to decide what question to ask at which point in time, we need to determine which question results in the greatest decrease in entropy. This is calculated using the information gain shown below:

$$IG = 1 - E$$

The feature that results in the greatest information gain will be selected for that branch of the tree. For continuous features, a threshold will be chosen based on what threshold will result in the greatest information gain. \\

Finally, we need to explain when the decision tree will decide when it should stop splitting. It will do this when a maximum depth $d$ has been reached or when there a fewer than $s$ samples remaining to split. \\

\heading{Pseudocode} With this, we can now construct our decision tree. The pseudocode for construction and inference can be found below. 

\begin{itemize}
	\item \textbf{S} is the training data with $m$ classes and $n$ continuous features. It is a set and all \textbf{S} are within $\{\textbf{S} : \textbf{S} = {((f_1,\dots,f_n),c),f_i\in\mathbb{R},c\in [1,m]}\}$\\
	\item $d$ is the maximum depth that the tree can reach and $s$ the minimum size of the data before we stop branching the tree.\\
	\item We also define the node object $\textbf{N}$. It has a children attribute $N.\sf{children}$ which is a list of $\eta$ child nodes $\{\textbf{N}_1,\dots,\textbf{N}_\eta\}$. Furthermore, it also stores a function $\textbf{N}.\sf{func}(\mathnormal{(f_1,f_2,\dots,f_n)})$ which takes in a list of features and outputs an integer $i\in[1,\eta]$. The depth of the node in the tree is also stored in $\textbf{N}.\sf{depth}$\\
\end{itemize}


\begin{figure}[h]
	\oneCol{.9}{
		\alg{$\sf{CalculateEntropy}(\bf{S}, t)$}
		$I\gets0$\\
		$\textbf{S}_1\gets\{((f_1,\dots,f_n),c) : f_i<t, ((f_1,\dots,f_n),c)\in \textbf{S}\}$\\
		$\textbf{S}_2\gets\{((f_1,\dots,f_n),c) : f_i>t, ((f_1,\dots,f_n),c)\in \textbf{S}\}$\\
		$\textbf{C}_1\gets\{c : ((f_1,\dots,f_n),c)\in \textbf{S}_1\}$\\
		$\textbf{C}_2\gets\{c : ((f_1,\dots,f_n),c)\in \textbf{S}_2\}$\\
		$I\gets1-\frac{|\textbf{C}_1|\times E(\textbf{C}_1)+|\textbf{C}_2|\times E(\textbf{C}_2)}{|\textbf{S}|}$\\
		Return $\textbf{S}_1,\textbf{S}_2,I$\\
		
		\alg{$\sf{BuildTree}(\bf{S}, \mathnormal{d, s})$}
		Initialise new node $\textbf{N}$\\
		Initialise empty queue $\bf{L}$\\
		$\textbf{L}.\sf{push}((\textbf{S},\textbf{N}))$\\
		While $\textbf{L}.\sf{size}\neq0$ do\\
		\ind $\textbf{S}, \textbf{N}\gets \textbf{L}.\sf{pop}()$ \\
		\ind If $\textbf{N}.\sf{depth} > \mathnormal{d}$ or $|\textbf{S}|<s$ then break \\
		\ind $I_{max}\gets0$ \\
		\ind For $i\in1\dots n$ do \\
		\ind \ind $E\gets 0$ \\
		\ind \ind For $t\in \{f_i : ((f_1,\dots,f_n),c)\in \textbf{S}_1\}$ do \\
		%\ind \ind \ind $\textbf{S}_{less}\gets\{((f_1,f_2,\dots,f_n),c) : f_i<t, ((f_1,f_2,\dots,f_n),c)\in \textbf{S}\}$ \\
		%\ind \ind \ind $\textbf{S}_{more}\gets\{((f_1,f_2,\dots,f_n),c) : f_i>t, ((f_1,f_2,\dots,f_n),c)\in \textbf{S}\}$ \\
		\ind \ind \ind $\textbf{S}_1,\textbf{S}_2,I\gets\sf{CalculateEntropy}\mathnormal{(\textbf{S}, t)}$ \\
		\ind \ind \ind If $I_{max} < I$ then \\
		\ind \ind \ind \ind $I_{max}\gets I; i_{max}\gets i; t_{max}\gets t$ \\
		\ind \ind \ind \ind $\textbf{S}_3\gets \textbf{S}_1; \textbf{S}_4\gets \textbf{S}_2$ \\
		\ind \ind \color{gray}{End if} \\
		\ind \color{gray}{End for} \\
		\color{black}
		\ind Initialise new nodes $\textbf{M}_l, \textbf{M}_r$ \\
		\ind $\textbf{M}_l.\sf{\textbf{p}}=\mathnormal{\frac{|\{c : c == 1,((f_1,\dots,f_n),c)\in \textbf{S}_1\}|}{|\textbf{S}_1|}}$\\
		\ind $\textbf{M}_r.\sf{\textbf{p}}=\mathnormal{\frac{|\{c : c == 1,((f_1,\dots,f_n),c)\in \textbf{S}_2\}|}{|\textbf{S}_2|}}$\\
		\ind $\textbf{N}.\sf{func}\gets\mathnormal{((f_1,\dots,f_n)}\mapsto \sf{If}\;\mathnormal{f_{i_{max}}<t_{max}}\;0\;\sf{then}\;1)$\\
		\ind $\textbf{N}.\sf{children}[0]\gets\mathnormal{\textbf{M}_l}$ \\
		\ind $\textbf{N}.\sf{children}[1]\gets\mathnormal{\textbf{M}_r}$ \\
		\ind $\textbf{L}.\sf{push}\mathnormal{((\textbf{S}_3,\textbf{M}_l))}$\\
		\ind $\textbf{L}.\sf{push}\mathnormal{((\textbf{S}_4,\textbf{M}_r))}$\\
		\ind \color{gray}{End for} \\
		\color{gray}{End while}\\
		\color{black}Return $\textbf{N}$\\ \\
		
		\alg{$\sf{Qry({f_1,\dots,f_n},\textbf{N})}$}
		$i\gets\textbf{N}.\sf{func}({f_1,\dots,f_n})$\\
		If $\textbf{N}.$$\sf{children}$$[i]=\bot$ then\\
		\ind If $\textbf{N}.\textbf{p} < 0.5$ then \\
		\ind \ind Return 0\\
		\ind else return 1\\
		Else \\
		\ind Return $\sf{Qry}$(${f_1,\dots,f_n},\mathnormal{\textbf{N}.\sf{children}[i]}$)\\
	}
	\vspace{-.5cm}
	\caption{Pseudocode for construction and inference of the tree.}
	\label{fig-rh}
	\vspace{-.5cm}
\end{figure}

\heading{Time Complexity} Now, let us analyse the time and space complexity of our algorithm. 

\begin{itemize}
	\item Let us assume that the tree will be fully filled with height $d$. This means that there will be $2^{d+1}-1$ nodes in the tree. This means the split would have been done $2^{d}-1$ times. In practice, this may not be true but let's assume so for the sake of simplicity.
	\item Splitting the tree has a time-complexity of $O(\mu\times|\textbf{S}|)$ where $|\textbf{S}|$ is the length of the data that still remains at that node and 
	$$\mu=\sum_{i=0}^{n}f_i\times|\{f_i : ((f_1,\dots,f_n),c)\in \textbf{S}_1\}|$$
	\item It should be reasonable to assume that on average each split $|\textbf{S}_{new}|=\frac{|\textbf{S}|}{2}$. Then, at depth $x$, the size of the dataset will be $|\textbf{S}_x|=\frac{|\textbf{S}|}{2^x}$ Then, the total average time-complexity is 
	$$O(\sum_{i=0}^{d}2^i\times \mu\times |\textbf{S}_i|)=O(\sum_{i=0}^{d}\mu\times |\textbf{S}|)=O(\mu\times |\textbf{S}|)$$
\end{itemize}

\section{Random Forest}
To improve the performance of our classifier, we make use of many different decision tree classifiers in an ensemble. \\

\input{fig/random_forest}

In essence, the data is split into multiple non-overlapping parts. A decision tree is trained on each part and together they form an ensemble. To obtain the final prediction, majority voting is used on the outputs of each decision tree. If more decision trees predict that the cells are benign, the prediction of the random forest will be benign. On the other hand, if more decision trees predict that the cells are malignant, the output of the random forest will be . \\
